{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "TWEET_Covid_classification(1stplace).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haythemtellili/Great-notebook/blob/master/TWEET_Covid_classification(1stplace).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18fEP8op7h8j",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1200/1*eCE-OsWxx7NYivrxt69QOw.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndl-0V_UPvdq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e88d093d-d1b0-4709-9f59-d13483a95521"
      },
      "source": [
        "! pip install pytorch-transformers\n",
        "! pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.18.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.14.9)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 15.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.5.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.9 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.17.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.15.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.0.0->pytorch-transformers) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.9->boto3->pytorch-transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=e37e0b6326cbd1df009998e8b9e36cc20d2fa1941551bd678b52f647e3785e66\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.43 sentencepiece-0.1.91\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "srxLFnm9KelR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import *\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import gc\n",
        "from scipy.special import softmax\n",
        "import os\n",
        "\n",
        "from fastai.text import *\n",
        "from fastai.metrics import *\n",
        "\n",
        "from fastai.callbacks import *\n",
        "from transformers import AdamW\n",
        "from functools import partial\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlE-F1PxKelr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 1024\n",
        "\n",
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) \n",
        "    np.random.seed(seed_value) \n",
        "    torch.manual_seed(seed_value)\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "        torch.backends.cudnn.deterministic = True \n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        \n",
        "seed_all(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw6D4PSaQ6e8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('/content/drive/My Drive/tweet/updated_train.csv')\n",
        "test=pd.read_csv('/content/drive/My Drive/tweet/updated_test.csv')\n",
        "sub=pd.read_csv('/content/My Drive/updated_ss(1).csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd8H-zx9Q8Ke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "4c12c158-b105-420f-b674-de5417804631"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>train_0</td>\n",
              "      <td>The bitcoin halving is cancelled due to</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>train_1</td>\n",
              "      <td>MercyOfAllah In good times wrapped in its gran...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>train_2</td>\n",
              "      <td>266 Days No Digital India No Murder of e learn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>train_3</td>\n",
              "      <td>India is likely to run out of the remaining RN...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>train_4</td>\n",
              "      <td>In these tough times the best way to grow is t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID                                               text  target\n",
              "0  train_0            The bitcoin halving is cancelled due to       1\n",
              "1  train_1  MercyOfAllah In good times wrapped in its gran...       0\n",
              "2  train_2  266 Days No Digital India No Murder of e learn...       1\n",
              "3  train_3  India is likely to run out of the remaining RN...       1\n",
              "4  train_4  In these tough times the best way to grow is t...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lsYW5kCT_8T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6c7a26c-33da-456a-994e-9561f9b5b320"
      },
      "source": [
        "train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5287, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "SMKSQ4bOKelz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "feat_cols = \"text\"\n",
        "label_cols = \"target\"\n",
        "\n",
        "train[\"text\"] = train[\"text\"].str.lower()\n",
        "train = train.drop_duplicates(subset=['text', 'target'])\n",
        "\n",
        "test[\"text\"] = test[\"text\"].str.lower()\n",
        "\n",
        "test[feat_cols] = test[feat_cols].astype(str)\n",
        "train[feat_cols] = train[feat_cols].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BD0G0MXKemQ",
        "colab_type": "text"
      },
      "source": [
        "## DATA SPLIT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxeoZfYMKemR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train.text, train.target, \n",
        "                                                    test_size=0.1, stratify=train.target, random_state=seed)\n",
        "\n",
        "train['is_valid'] = True\n",
        "train.loc[X_train.index, 'is_valid'] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QHMSsN9KemY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "b76eb37c-4fc0-49eb-d6f2-6d75c25da9aa"
      },
      "source": [
        "# Creating a config object to store task specific information\n",
        "class Config(dict):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "    \n",
        "    def set(self, key, val):\n",
        "        self[key] = val\n",
        "        setattr(self, key, val)\n",
        "        \n",
        "config = Config(\n",
        "    testing=False,\n",
        "    seed = 2019,\n",
        "    roberta_model_name='roberta-base', # can also be exchnaged with roberta-large \n",
        "    max_lr=2e-5,\n",
        "    epochs=2,\n",
        "    use_fp16=False,\n",
        "    bs=4, \n",
        "    max_seq_len=64, \n",
        "    num_labels = 2,\n",
        "    hidden_dropout_prob=.05,\n",
        "    hidden_size=768, # 1024 for roberta-large\n",
        "    start_tok = \"<s>\",\n",
        "    end_tok = \"</s>\",\n",
        ")\n",
        "\n",
        "class FastAiRobertaTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around RobertaTokenizer to be compatible with fastai\"\"\"\n",
        "    def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs): \n",
        "        self._pretrained_tokenizer = tokenizer\n",
        "        self.max_seq_len = max_seq_len \n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self \n",
        "    def tokenizer(self, t:str) -> List[str]: \n",
        "        \"\"\"Adds Roberta bos and eos tokens and limits the maximum sequence length\"\"\" \n",
        "        return [config.start_tok] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [config.end_tok]\n",
        "    \n",
        "# create fastai tokenizer for roberta\n",
        "roberta_tok = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "fastai_tokenizer = Tokenizer(tok_func=FastAiRobertaTokenizer(roberta_tok, max_seq_len=config.max_seq_len), \n",
        "                             pre_rules=[], post_rules=[])\n",
        "\n",
        "# create fastai vocabulary for roberta\n",
        "path = Path()\n",
        "roberta_tok.save_vocabulary(path)\n",
        "\n",
        "with open('vocab.json', 'rb') as f:\n",
        "    roberta_vocab_dict = json.load(f)\n",
        "    \n",
        "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))\n",
        "\n",
        "# Setting up pre-processors\n",
        "class RobertaTokenizeProcessor(TokenizeProcessor):\n",
        "    def __init__(self, tokenizer):\n",
        "         super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class RobertaNumericalizeProcessor(NumericalizeProcessor):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, vocab=fastai_roberta_vocab, **kwargs)\n",
        "\n",
        "\n",
        "def get_roberta_processor(tokenizer:Tokenizer=None, vocab:Vocab=None):\n",
        "    \"\"\"\n",
        "    Constructing preprocessors for Roberta\n",
        "    We remove sos and eos tokens since we add that ourselves in the tokenizer.\n",
        "    We also use a custom vocabulary to match the numericalization with the original Roberta model.\n",
        "    \"\"\"\n",
        "    return [RobertaTokenizeProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]\n",
        "\n",
        "# Creating a Roberta specific DataBunch class\n",
        "class RobertaDataBunch(TextDataBunch):\n",
        "    \"Create a `TextDataBunch` suitable for training Roberta\"\n",
        "    @classmethod\n",
        "    def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
        "               pad_first=True, device:torch.device=None, no_check:bool=False, backwards:bool=False, \n",
        "               dl_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
        "        \"Function that transform the `datasets` in a `DataBunch` for classification. Passes `**dl_kwargs` on to `DataLoader()`\"\n",
        "        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n",
        "        val_bs = ifnone(val_bs, bs)\n",
        "        collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
        "        train_sampler = SortishSampler(datasets[0].x, key=lambda t: len(datasets[0][t][0].data), bs=bs)\n",
        "        train_dl = DataLoader(datasets[0], batch_size=bs, sampler=train_sampler, drop_last=True, **dl_kwargs)\n",
        "        dataloaders = [train_dl]\n",
        "        for ds in datasets[1:]:\n",
        "            lengths = [len(t) for t in ds.x.items]\n",
        "            sampler = SortSampler(ds.x, key=lengths.__getitem__)\n",
        "            dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
        "        return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\n",
        "\n",
        "class RobertaTextList(TextList):\n",
        "    _bunch = RobertaDataBunch\n",
        "    _label_cls = TextList\n",
        "    \n",
        "    \n",
        "feat_cols = \"text\"\n",
        "label_cols = \"target\"\n",
        "\n",
        "processor = get_roberta_processor(tokenizer=fastai_tokenizer, vocab=fastai_roberta_vocab)\n",
        "\n",
        "data = RobertaTextList.from_df(train, \".\", cols=feat_cols, processor=processor) \\\n",
        "        .split_from_df('is_valid') \\\n",
        "        .label_from_df(cols=label_cols, label_cls=CategoryList) \\\n",
        "        .add_test(RobertaTextList.from_df(test, \".\", cols=feat_cols, processor=processor)) \\\n",
        "        .databunch(bs=4, pad_first=False, pad_idx=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9Dok5uOKemg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining our model architecture \n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self,num_labels=2):\n",
        "        super(CustomRobertaModel,self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.roberta = RobertaModel.from_pretrained(config.roberta_model_name)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "        \n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _ , pooled_output = self.roberta(input_ids, token_type_ids, attention_mask) \n",
        "        logits = self.classifier(pooled_output)        \n",
        "        return logits\n",
        "    \n",
        "roberta_model = CustomRobertaModel()\n",
        "CustomAdamW = partial(AdamW, correct_bias=False)\n",
        "\n",
        "seed_all(seed)\n",
        "\n",
        "learn = Learner(data, roberta_model, metrics=[accuracy, error_rate, FBeta(beta=1, average='macro')]) # \n",
        "learn.callbacks.append(ShowGraph(learn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvGV0IMQKemo",
        "colab_type": "text"
      },
      "source": [
        "## MODEL TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGhn21L6Kemr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e0426ad9-26d7-4add-f4e4-08a6a63d854e"
      },
      "source": [
        "learn.model.roberta.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(514, 768)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (6): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (7): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (8): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (9): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (10): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (11): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4o7cvyoTzF4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "03b6bbff-f215-4614-a6f3-4b8157f6f1cf"
      },
      "source": [
        "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>f_beta</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.255786</td>\n",
              "      <td>0.204430</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.913036</td>\n",
              "      <td>03:18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.189331</td>\n",
              "      <td>0.197714</td>\n",
              "      <td>0.933837</td>\n",
              "      <td>0.066163</td>\n",
              "      <td>0.933803</td>\n",
              "      <td>03:18</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXhU1fnA8e87k31hycKWAAn7LruggiCILBakLmDVqlVxrVatFtuqlLrQ+qu1tli1Lq0LUsRaqaAoioAgSED2fQkk7ATCmpDt/P6Y7U4yk5lAwiTD+3mePNy598zMmUvy3jPnnvMeMcaglFKq7rOFugJKKaWqhwZ0pZQKExrQlVIqTGhAV0qpMKEBXSmlwkREqN7YHlffdGrXmugIvaYopVSwVqxYcdgYk+rrWMgCekT9Rrwz6yt6tGgYqioopVSdIyK7/B0LafNYR8ArpVT1CW1A10lNSilVbUIa0ItLNaArpVR1CVkfOkBhcSnzNx3kTEkZw7s0CWVVlFJ1QHFxMbm5uRQWFoa6KjUuJiaG9PR0IiMjg35OSAP6sYJiHpq+CoDsKaNCWRWlVB2Qm5tLYmIiGRkZiEioq1NjjDHk5eWRm5tLZmZm0M8LqstFRIaLyGYR2SYiE30c/7OIrHL+bBGR/GBe1xXMATImzubTNXuDrrhS6sJTWFhIcnJyWAdzABEhOTm5yt9EArbQRcQOTAWuBHKB5SIyyxizwVXGGPOwpfzPgR5VqoXTA9N+4OipIvJOFXGsoJhb+rWkVWrC2byUUipMhXswdzmbzxlMl0tfYJsxZofzTaYDY4ANfsrfCDxdlUrMe2QgJwpLGPvKEp78ZL17/9uLs7mxbwviouyUlhkaxEXywOA2HCsoJik+6oL5j1VKqWAEE9DTgBzL41zgYl8FRaQlkAl87ef4BGACQFSTNu79LZLiiYqw8f2vh3Dr28u5ODOJoR0b888lO/ng+91er/HSvK0AtG+cyANXtOFHFzXzOl5WZrDZNNArpapffn4+06ZN47777qvS80aOHMm0adNo0KBBDdXMobpvio4HZhpjSn0dNMa8DrwO0KNXL3MU6JuRRJRz+n+jejF89tAAd/nL2qaQlX2E1bnHGN6lCQs2H2JvfgE5R0+zKiefn3/wAz//4AcAxvZI4+Mf9gCweOIVTPlsEw1iI5k8prO25JVS1SI/P59XXnmlQkAvKSkhIsJ/OJ0zZ05NVw0ILqDvAZpbHqc79/kyHrg/mDe2iwQ1sqV3RhK9M5IA+MnFLdz7i0rK+Nv8bbz8laPF7grmAJdO8XxBeHfpLh4Y3IZHh7XTwK6UOicTJ05k+/btdO/encjISGJiYmjYsCGbNm1iy5YtXHPNNeTk5FBYWMhDDz3EhAkTAMjIyCArK4uTJ08yYsQILrvsMpYsWUJaWhqffPIJsbGx1VI/CTRbU0QigC3AEByBfDnwE2PM+nLlOgCfA5kmiCmgvXv3NllZWWdbby/Zh0/xwfe7aZ2aQEJMBPe9v9JnuReu68Zbi7N5746+JCdEV8t7K6XOn40bN9KxY0cAfve/9WzYe7xaX79Ts3o8/aPOfo9nZ2dz9dVXs27dOr755htGjRrFunXr3EMLjxw5QlJSEgUFBfTp04cFCxaQnJzsFdDbtGlDVlYW3bt354YbbmD06NHcfPPNAT+vi4isMMb09lU+YAvdGFMiIg8AcwE78JYxZr2ITAayjDGznEXHA9ODCebVLSMlnidGej509pRR7Dx8iozkOH7z33VMW+boh39s5hoAej0zj/fuuJjL2qac76oqpcJI3759vcaJv/zyy3z88ccA5OTksHXrVpKTk72ek5mZSffu3QHo1asX2dnZ1VafoPrQjTFzgDnl9j1V7vGkaqtVNchMiQfgyVGdqBcTyasLtnsdv/nNZSx9YghN6seEonpKqXNUWUv6fImPj3dvf/PNN8ybN4/vvvuOuLg4Bg0a5HMceXS0p3fAbrdTUFBQbfUJ6UzR8yE2ys7EER24uFUSrVLi2ZNfwE/+sQyAfs9/pTNUlVJBS0xM5MSJEz6PHTt2jIYNGxIXF8emTZtYunTpea7dBbRi0eD2jWiZHM8lrVP44ckr3fszJs6muLQshDVTStUVycnJXHrppXTp0oXHHnvM69jw4cMpKSmhY8eOTJw4kX79+p33+gW8KVpTqvOm6Nl4fs5GXlu4A4C7B7by6oNXStVOvm4ShrOq3hS9YFro5VkDuCuwK6VUXXbBBnSANZOGUT82kgibUFDkcy6UUkrVGRd0QK8XE8lL47pTUmaYNGt94CcopVQtdkEHdIA+mY5ZqP/OyglQUimlarcLPqAnREfw4JC22ASOnioKdXWUUuqsXfABHWBQ+1TKDMzbeCDUVVFKqbOmAR24KL0BjetF8/Wmg6GuilIqjCQkOBbo2bt3L9ddd53PMoMGDaK6hnBrQAfsNqF3RhJr9xwLdVWUUmGoWbNmzJw5s8bfRwO6U9e0+uQeLdB+dKWUXxMnTmTq1Knux5MmTeKZZ55hyJAh9OzZk65du/LJJ59UeF52djZdunQBoKCggPHjx9OxY0fGjh2ruVxqQte0+gCs3XOMge1SQ1wbpVRAn02E/Wur9zWbdIURU/weHjduHL/4xS+4/37Hsg8zZsxg7ty5PPjgg9SrV4/Dhw/Tr18/Ro8e7Xf9hb///e/ExcWxceNG1qxZQ8+ePaut+hrQnbo004CulKpcjx49OHjwIHv37uXQoUM0bNiQJk2a8PDDD7Nw4UJsNht79uzhwIEDNGnSxOdrLFy4kAcffBCAbt260a1bt2qrnwZ0p/pxkbRMjmOd9qMrVTdU0pKuSddffz0zZ85k//79jBs3jvfff59Dhw6xYsUKIiMjycjI8Jk293zQPnSLLmn1WZOrAV0p5d+4ceOYPn06M2fO5Prrr+fYsWM0atSIyMhI5s+fz65duyp9/sCBA5k2bRoA69atY82aNdVWNw3oFh0aJ7Inv4DCYs3ropTyrXPnzpw4cYK0tDSaNm3KTTfdRFZWFl27duWdd96hQ4cOlT7/3nvv5eTJk3Ts2JGnnnqKXr16VVvdtMvFomkDx0Kt+48VkpESH6C0UupCtXat52ZsSkoK3333nc9yJ0+eBByLRK9btw6A2NhYpk+fXiP10ha6RTPncnR7j1XfMCKllDpfNKBbuNYXnbkil1At/KGUUmdLA7pF0/qOLpf/rNzDhytyQ1wbpZQvF0pj62w+Z1ABXUSGi8hmEdkmIhP9lLlBRDaIyHoRmVblmtQCsVF29/a2gydDWBOllC8xMTHk5eWFfVA3xpCXl0dMTEyVnhfwpqiI2IGpwJVALrBcRGYZYzZYyrQFngAuNcYcFZFGVapFLXTqTEmoq6CUKic9PZ3c3FwOHToU6qrUuJiYGNLT06v0nGBGufQFthljdgCIyHRgDLDBUuYuYKox5iiAMabOpy3UJemUqn0iIyPJzMwMdTVqrWC6XNIA63I+uc59Vu2AdiKyWESWishwXy8kIhNEJEtEsmr7FfZUkbbQlVJ1S3XdFI0A2gKDgBuBf4hIg/KFjDGvG2N6G2N6p6bWznwp3Zs7qp13UrMuKqXqlmAC+h6gueVxunOfVS4wyxhTbIzZCWzBEeDrnNducczaiovWOVdKqbolmIC+HGgrIpkiEgWMB2aVK/NfHK1zRCQFRxfMjmqs53nTuF4MV3drysIthygtC+876Uqp8BIwoBtjSoAHgLnARmCGMWa9iEwWkdHOYnOBPBHZAMwHHjPG5NVUpWtahM2Rx3ju+v0hrolSSgUvqH4FY8wcYE65fU9Ztg3wiPOnzntiZEf+u2ovu4+cDnVVlFIqaDpT1IfG9RyD+ad8tokcDepKqTpCA3oAc9buC3UVlFIqKBrQAygsLgt1FZRSKiga0AM4U6IzRpVSdYMG9AC0ha6Uqis0oPuRkRwHQFmYZ3VTSoUPDeh+vHlbHwBKyrSFrpSqGzSg+9E6NYHMlHh2HDoV6qoopVRQNKBXYufhUyzZnsea3PxQV0UppQLSgB4EbaUrpeoCDehKKRUmNKBX4tOfXwbA0dOaG10pVftpQK9Ep6b1sNuEwyfPhLoqSikVkAb0SthsQmmZYer87RQW64xRpVTtpgE9SNpKV0rVdhrQA7imezMAjpzSfnSlVO2mAT2An16SAeii0Uqp2k8DegDJ8VEA7DisY9GVUrWbBvQAkhOiAXhh7qYQ10QppSqnAT2AhGjHsqvtm9QLcU2UUqpyQQV0ERkuIptFZJuITPRx/DYROSQiq5w/d1Z/VUNnYLtU0DS6SqlaLiJQARGxA1OBK4FcYLmIzDLGbChX9N/GmAdqoI4hlxIfxfaDJ0NdDaWUqlQwLfS+wDZjzA5jTBEwHRhTs9WqXZITonTYolKq1gsmoKcBOZbHuc595V0rImtEZKaINPf1QiIyQUSyRCTr0KFDZ1Hd0EhOiKaguJTTRSWhropSSvlVXTdF/wdkGGO6AV8C//JVyBjzujGmtzGmd2pqajW9dc1zDV3UsehKqdosmIC+B7C2uNOd+9yMMXnGGNfc+DeAXtVTvdohxTl0Uaf/K6Vqs2AC+nKgrYhkikgUMB6YZS0gIk0tD0cDG6uviqGXnKAtdKVU7RdwlIsxpkREHgDmAnbgLWPMehGZDGQZY2YBD4rIaKAEOALcVoN1Pu9ck4vyTmkLXSlVewUM6ADGmDnAnHL7nrJsPwE8Ub1Vqz1cfeiHtYWulKrFdKZoEGIi7cRH2bXLRSlVq2lAD1JyQjRHtMtFKVWLaUAPUnJCFHk6uUgpVYtpQA9Scny09qErpWo1DehBSkmIIk/HoSulajEN6EFydbmUlWnWRaVU7aQBPUgtkuIoLTNs1ayLSqlaSgN6kGwiANz9blaIa6KUUr5pQA9SibOrJTvvdIhropRSvmlAD9Kobo50NYPb150skUqpC4sG9CDVi4kEYP7mupPHXSl1YdGArpRSYUIDehU8OKQtACWlZSGuiVJKVaQBvQpSEx1pdI+c1hmjSqnaRwN6FaS40uie0ICulKp9NKBXQUqiLnShlKq9NKBXgS4WrZSqzTSgV0HjejGIwO4jOrlIKVX7aECvgvjoCFITotmbXxDqqiilVAUa0Kvo4IkzTF+ew668U6GuilJKedGAXkVREY5TdvkL34S2IkopVU5QAV1EhovIZhHZJiITKyl3rYgYEeldfVWsXf50/UXu7aISnWCklKo9AgZ0EbEDU4ERQCfgRhHp5KNcIvAQsKy6K1mbpCREu7cfmLbSvZ1z5DQvzdvCbs3GqJQKkWBa6H2BbcaYHcaYImA6MMZHud8DfwAKq7F+tU5KQpR7+4sNB9zbz83ZyEvztjLwhfmhqJZSSgUV0NOAHMvjXOc+NxHpCTQ3xsyu7IVEZIKIZIlI1qFDdTNrYZP6MV6PTxeVVChTrLlelFIhcM43RUXEBrwIPBqorDHmdWNMb2NM79TUuplXPDEmkt+P6ex+/PjMNYB3oD90QmeSKqXOv2AC+h6gueVxunOfSyLQBfhGRLKBfsCscL4xekv/DC5rkwLAp2v2cex0sdcN0s37T4SqakqpC1gwAX050FZEMkUkChgPzHIdNMYcM8akGGMyjDEZwFJgtDEmrBffrB8b6d6++c1lvL9st/vx2j3HQlElpdQFLmBAN8aUAA8Ac4GNwAxjzHoRmSwio2u6grVVfLTdvW0N4LGRdo6c0lwvSqnzLyKYQsaYOcCccvue8lN20LlXq/aLi/J96po1iOHgibAe6KOUqqV0puhZurJTY5/7E6IjmLfh4HmujVJKaUA/a5e2SeH+wa3dj5Pio/jhySvZfOAERaVlnDpTcTijUkrVJA3o5+Cxqzpw14BMAFY+eSUN46P45bD2AOQc1RmjSqnzSwP6OfrNqE5kTxnlftwtvQGgy9Qppc4/DejVzJUaYNP+4+w7pnnTlVLnjwb0apbqXHf0mdkbufaVJSGujVLqQqIBvZolREdgtwkAe49VHL7YY/IXZEycTWmZOd9VU0qFOQ3o1UxEvIJ1YXGp1/Gjp4sB+PV/1p7Xeimlwp8G9BqQ1iDWvW1N1FVmCfT/zsrh3e+yeWPRjvNZNaVUGAtqpqiqmsQYz2k9fPIMzZPiADhZLtXuk5+sB6BXy4b0aNHw/FVQKRWWtIVeA6IjPKf18EnP8MVjzu6W8rYdPFnjdVJKhT8N6DUgOsKTuGv7oZNsOXCCKZ9tYsAfHasZjevd3Ku8NegrpdTZ0oBeA67o2Mi9PeWzTQz780JeXbDdvW94lyZe5T9ZtYcThb5b70opFSwN6DXg7oGtWDzxCq++dCvXWHWXTfsdLXillDoXGtBrgIiQ1iCW1IRon8eti2O4WBfIMMaQMXE2j85YXWN1VEqFHw3oNSgl0XdArxcbySWtkwGItIt7/8HjjolIry90DGX8aGVuDddQKRVONKDXoHTLeHSXLx8eSP3YSN6+vQ9rJw1j7aSr3Mc2Odcifb4K3S/Pf7aRT1btCVzQ6R8Ld3Dnv5YHXV4pVXdoQK9B7Zskej1++kedaNvYsS86wk5iTCQxkXYeu8qRcteVzKtVarz7OScKi5mzdh9T52+r8PoT3snitQU7eGj6qqDr9OycjczbeNBrUWulVHjQgF6DGsY5Mi+2SIrj4/su4fZLM32Wu+Myx37XrNKUeE9XzYHjhdz3/kpemLuZYwWOkTArdh2huLSMLzYc8Pl6hcWlzFq9l5JSR9DedvCE14xVgN1HNF+7UuFGZ4rWINcol/ZNEiudCRoTaScxJoJDJ86Qd/IM32cfcR8b+uJC9/btb3/PfYPacOc7WdzY13ssuzEGEUd/fIcnPwdgx9C2/GJoO4a+uJD6sZGs+O1Qd/m8k2do0yjh3D+kUqrW0BZ6DUqMcYxmCWaMeWpiNP/6bhe9npnnt8zK3fnc/d4KAD74Psfr2C1vfs/jM1djjCdfzEvztlLsbKUfKyjm1BlPojCdzKRU+AkqoIvIcBHZLCLbRGSij+P3iMhaEVklIt+KSKfqr2rd07lZPQBu6ZcRsGy9mIpDGctrEBfJmO7NAGiZHOeVYuDbbYeZkZXLkVPegXq5pbWfX+A5lnfKuwtGKVX3BQzoImIHpgIjgE7AjT4C9jRjTFdjTHfgj8CL1V7TOqhhfBTZU0YxqlvTgGWtwRmgS1o9+mYmee3LP13Mf1bucZePstu4sW8LrzLZed594y/N2+re/rFlwY2Dx2t/QC8uLeM3H69lb/7Zrfx0pqSU73ceCVxQqTARTAu9L7DNGLPDGFMETAfGWAsYY45bHsYDunpDFUVH2r0ePze2Kx/c1Y8rOjTyWX5vfiGni0tpGBfp/iYA8O/ljglKrouBNaDlWVrv6/Yeq7a615RVOfm8v2w3j8wIfhSP1V+/2sYNr33Hyt1HMcawYe/xwE9Sqg4LJqCnAdYO21znPi8icr+IbMfRQn/Q1wuJyAQRyRKRrEOHDp1NfcNWiyTPmPW0BrF0S2+A3Sb8YmhbADqUGwJ58kwJpWWG2Eg76y2BKmvXUQBuuyTD73td3i6VffkVV1PyZdvBE2RMnM1m5xj56vDqgu2s2xP4ghLrvMiV70YK1sETjs/40ze/581vdzLy5UU8P2fjWb2WUnVBtd0UNcZMNca0Bn4F/NZPmdeNMb2NMb1TU1Or663Dwg2WDIxXdfYk72pa3xHoL22T4t5nDdaxUXY6NfW00A85u1IGtfc+v78b3dm93TI5LugFrOeudwyN/OPnnslOOw6ddM9qrYq9+QVcOuVrpny2iav/+m3A8q77u67hmlWV1sCZh/5MCc/MdgTy1xbu8LpxrFQ4CSag7wGsY+TSnfv8mQ5ccy6VuhB1S2/AzudH8tK47vzyqnbu/amJ0Sx6fDATR3Tgg7v60atlQ8b28HxBio2y88frurkfnzhTggjERHh34VzXK9293aR+DMcLSzhdbsGN8owxvL04G4CvNh1kdU4+hcWlXPGnBfR97is27a9aF8ana/aypwr94XudF53808XM33SQwyer1u/fMN73jeZ8P3nplarrggnoy4G2IpIpIlHAeGCWtYCItLU8HAVsRVWZiHBNjzTiorynBzRPiiPSbqN/62Q+uvcSLmrewH2ssLiMLmn1yZ4yyt0tExtpx2YT/npjD3e5+OgIbu3fkovS69O0fgwAr8zfTmUWb8vzCqJjpi5m9pp97scb9x131qGUk2cqvziAY7y9P4XFpbT59Rz+Y8lfc/e7jiGaZ0rKuP2fyxn7ymLOlJQy5bNNHA8wFNQYw3fb83weq8pFRam6JGBAN8aUAA8Ac4GNwAxjzHoRmSwio53FHhCR9SKyCngEuLXGaqwATwrexvU8s0ob13MEatcF4UcXNWPhY4N5746LAfjdmC588sBlNKnn6Mb5m490AlZ78ivOJv10zV73dnGpo+viR3/9li5Pzw1Y56ecS+5ZFRSVMuZv3zL9+92UlBmfZVxyjhTw41eW8OqC7fxp7uZK3+vbbYf5bN1+r33XOId8akBX4SqomaLGmDnAnHL7nrJsP1TN9VIBLJl4BV+sP8DIrp7+dldwj4vytIRbJMfRIjnO67npDSsmDfPlVx+tBWDmPf15ZvZGjhcWc0WHRszf7Lih/fjMNZwoLGFrJUvo/fWrrcRHR1RY1AOgtMzw7tJsVuceY3Wu4yaptaU/tkcaH//g3bvnugG8J7+QA8cL3Rex8k4WVvzG4Cqbo2kPVJjSmaJ1VKTdxqhuTd3T/cEzKuRAgBuWzZPi6JZev8LYd38SYyK5ODOJ3KMF/H629yiR33+6wb29eNvhCs/905dbmPzpBn710ZoKxy5+bp67le+LiGPEjy/zNh7g4ue+IvvwKZ/HbTapsG9092bUi4ngmdkbgxplo1RdowE9jFzjvFl6JohMild1bsKZkjLmbzrIZ2v3sf+Y/4tAcWkZ6UlxFJWUVZql8aY3lvHPxTs5XVTCsYJibnpjqfvYoq0Vg/3hk0Xui5AvRSVlREfaWPT4YJY+McRnmS0HfA+nLCjypDlY8duhfPrzy+jcrD7HnS33YEbZKFXXaHKuMNIyOT5wISdXH/zt//TkRl/+m6EVlscDR4s+r9xY8O9/PYS+z31Voeyk/23gyKkiXv668v55F183NwuLS4mJtFNUUkaU3UbzpDgfz3TwNaSxrMywOjff/TgpPopk5+pRj13Vnhec/e+lZQZ7uZZ8aZlh9tp9DO/chKggv8EoVVvob2wYaeBc2i4mMvB/q6++5+w87+6LS1on07tlQ+rHRtIqxXOx6NS0Ho389F0DrMr1353x4g0XMaBtCvcPbg3Apn0niLLb+Mv47u4yrtEzZ0rKvLqFrNkiXXwF9BlZOe7hlr8e2cGrW+r+wW3cWSZ9pRT4ZvNBHvzgh4A3jJWqjTSghxGbTXj5xh7MfnBAwLJdLOkCXK5/9Ttufet79+OSUkOEc4m85klxNIwL7oKxcItnFvCDQ9p6dZf8uGc6795xMQPbOiY+fb5+P0WlZYzpnsbMe/oDMPaVJWzcd9zRQrcE9GQfa7RmZR+tsM+a6/2Oy1pVOD55TOcK5cAx1PGOf2UB8P1O30MelarNNKCHmdEXNaN1auA850nxUT73L7AE4+KyMiLtnl+RNOfomFjnKJoFjw3i7oGOgJmZ4ru755Er29GkfsXWfLqPbhRrl9GIvyyiqLQsYLfH5+v3V9iXYgn85btUwLHgCFQM6JlPeAZyLd1xhFmr96JUXaIB/QJl7YYob8Pe4xQUlfLD7nyvm5muNASuWagtk+O5uJUjCZivWaeDLekHPrirHy+N83SrNPHRZZOaGE39WM/szhW7jrJ4W+CWclmZ90gZX0HcyvU5nvjP2krLPfjBDwHfW6naRAO6qmDky4t85npp5mxpx1jHuSc5WtWHTxaRPWUUiyde4T5mvcHav3WyexQO+A+6Xz96eaV1W/DYIKbddbHXPmvOd8C9qIc/vt77VBAzXZWq7TSgX8A+/8UAHrnSkzdm7aRh7u33ljrS8D43tqt7X1PnmPBSy9hxV/dFlLNrppmle6UsyBxYn/78Mvd2ckI0Xz480G/ZlsnxXNI6hXoxEfTJcCzrN+71pV5lXMM2Z9zd3+/rPHhFG2ziGBp5vLCYrzYddB/bMPkqbALJfrqllKqtNKBfwDo0qceDQzxpeBJjInnsqvYAvLV4J4DXuqOuHDBHT3uGMEZF2HhubFc+uvcSwNGV89CQtqQkRPP48PaVvv/qp4ex6qkr6ZJW32t/m0YJ7r5zV33KWzPpKl647qIK+/fmF7gzQfZu6X8d15bJ8ZQZyD16mvvfX+nuXmmdGk9cVAQPXNGWo6eLOFNS6vc1lKptdBy64vdjOrvHesdHeU/0aRDn6dN2zdq0BnSAn1zsvWrSw1e242FLy98fa3+5lYjQtH4Mu/JOV9of3swyi9TVzXLJlK8BiLCJz9miLq50CFf8aYHXZ3ZdQDKS45wBvyCom8xVVVpm2HO0oEJaBqXOhbbQFbf0z2BQe8fKSCO7ei+XZw26ri6XI6dqPv1sI2f/e0QlQTkqwsbVzuX9dh4+RUGxpzVdEqC/p6VllI11Zq1r8WzXiJtdeb5TC5yrNxbtYOAL8/3OdFXqbGhAV14a1YuhVapn+KA1oLuC7G2XtKzxerhuqAYaseLK8/75uv2cKQ6c8qD864N38D94wpEuOMPZcv7ZP7PYcch/8rGzUVpmeP4zx4Ihuiyeqk4a0FUFL4/35FG35jCPtNvY+fxIHriira+nVatGiY7++tIALe0BzglKL365hTlr91Va1srfsM07Ls0EvMfpWydKBVJYHLjP3bqk3qETtX+xblV3aEBXFZS/SWlV2fj16uRqQQcKeNYW/NOz/OdS9yUxuuItpHqxjn3WzznpfxsqlPNl8/4TdHjycz5fV3GyEzhSGny+br/XRepQFVdhUqoyGtCVT78d1ZFxvZsHLlhDXDdgA61MBBUD8+QxnVn55JUBn7foV4Pd79Osfgz3DmrtFcg3TL6qKlV251m/570VTPJxcRnxl0Xc894KSsosffbaQlfVSAO68unOAa34g2Wt0vNtRNcm/LR/S+4b1CZg2dc0wTkAABorSURBVJRyGSIHt2/kN7WBVYO4KAa0dSy+3TcziV8N7+B1PC4qwj308v1luwK+Xly0p3vqn0uyKfEzwWlXniflwIETVV9sWyl/NKCrWik6ws7kMV0qTZ3rUj6NQFXS3u5wLpDx31W+87a4brp+s9l3P/qhE2fcueTLL9axbKf3DFbXENCvLZOY9uZrQFfVRwO6qvMmjvBuWVsTigXy21EdKz3eKDGG7s0b+MxVA9Dn2Xn0e/4rth44UaFFXj4dsateb367071v5+FTunqSqjYa0FWdd1HzBu6ZqgCR9uBv3HZ13gBunuR/ndUWSXFsP3gKY/yPuFm5+2iFHDK787yzOZ4uly+ml3Mm65LtFVdzUupsBBXQRWS4iGwWkW0iMtHH8UdEZIOIrBGRr0Sk5gcqK2XRwtI1kxjjewaqLyLCh/f058O7L/FbplfLhuw/XkjWroq5111OnSl1d7ncfmkGAK8t3OE+/u7SXZwq8h7SeNeAVojA6aJSjDG6eLU6ZwGn/ouIHZgKXAnkAstFZJYxxjqW6wegtzHmtIjcC/wRGFcTFVbKl5SEs0+k1ScjyfPgwAZY8jKIHWw2sEVwbZGhLGIvCfM/g7QGzmN2EDuPx2RzutjQcsNCGtWP4077Ae6LbUeZfQel2DizdB/RkZGs+d86fmyzUT8+hkOnSinFRsbBfK6L383e5Rt4bLmN/SeKmTiyE13Sk7zeA5ut3GM7iM3yOCJAWed+FfaCyeXSF9hmjNkBICLTgTGAO6AbY+Zbyi8Fbq7OSioViIjwk4tbcM6j5AuOQPZiMKVQVgplJcSbUq61nyE6B9hjPMdMKfcBRAJ7HT9dI4Fv4XeuLwmfO/55wfW4GHBdexbCCwCunpgoYN65foBK+Av0Pi8WEZVcQALt9/GaPp8b4AIU6L1sEUG/xrHCUuJjo4mwR3rtX5qdz6//u4GZ911GUkJc1epXCwUT0NOAHMvjXOBiP2UB7gA+83VARCYAEwBatGjhq4hSZ82a6vesZVwGD3svfCHANX/6hvaNE/nV8A68tnAHvxvdmagIG5kT/4edMjo2jmd0tyb85cuNzH9kAEXFxYz56yJiIwzzHxnAsBfnE2UzvPnTntz65nfYKeOVG7uzce9R/rFgK3bKsFHGqC6Nub1/c+dFo8zr4uH517F/8dYDfPJDDndd0oK2qbFgyvyWrfr+kgBlXe9V5Gf/2bxmzWW29DdVrh/wdTTw5lm8aFAXoojgLk7BXiBtlYfsas22KCI3A70Bn6sUGGNeB14H6N27d5DZspUKvRZJcWzYd5yhLy6gpMxwWZsU2jRKwGCjBBvZ+aU8++UuIA57fBLN4iI5TH0ogX2ksKMkhaeu7kTTVhlsM472ka1ZN+rHnmb1N54/hdbRzaFVcOP/Vx7eyozSLcxYBNMn9OPA8ULGdE8L/ESnYwXFjPzLIp4Y2YGruzWr0vmoCfM2HKB/q4bER0pwF6CyEvcF5Ko/z8dOGfWibUy/s69X2dLSEm59cyl2yvjVsLZ0ahLvPv7v77NZvPUg1/ZoyuVtkpyvGczFqQr7vV6zGi6QlQgmoO8BrFMG0537vIjIUOA3wOXGGJ3+psJKt/QGXmPR75+20msBjROWESyRdkFEuKF3OjOychnwR0ePZO7RAq+UvvVjI4mwea/FumbPMfJPF9EgLvA9gcaWxUTGOxf52H7wJI8MqzwPvcvqnHz25BcwadaGkAf0wyfPcOc7WfRrlcT0Cf3BHvyN7Y9W5LLZOL/xFwLpvbyOl5aU8W2ZYwjpgs/hht5p/NGZS3//vq3M2ryFJvGtuLxH5UNYQ80Yw3tLdwGZfssE0xG0HGgrIpkiEgWMB2ZZC4hID+A1YLQx5qCP11CqTuvYJLHCvhte+w6gQooE13jzwc6UxC4jujYBYJBzrdV6MRFeOd0Htktl477jdJ/8JeBYp/VXM9e4F+woz9cwype/3hbU5wFP7pvDtSCfTIFzBNDSHUcClKxo2U7PurMRNqmwKEn5BG8zsnLJc37mUuc53HfM/wQvYwxz1+8PmCiupq3fe5wnP6k8X1HAgG6MKQEeAOYCG4EZxpj1IjJZREY7i70AJAAfisgqEZnl5+WUqpNaVbLIRc+WDbweuwJ6x6b1vPa7RtP8/aZezP/lICLsNuw2YekTQ/jo3v7c0s8z2vdEYTGz1+zj31k59H3uK58rJ5WfmepS2Xh5q5HOC0xVnlNTrDnpq8p6USwpM3y3PY/tlpTHpT4+2xvOyV2u87q7krz3c9cf4O53V3DVSwvPuo7VIZi8RkHdqjXGzDHGtDPGtDbGPOvc95QxZpZze6gxprExprvzZ3Tlr6hU3dKykpWFbrC00N++rY87A2RGiqc75eJMz9DI2Cg7mZZjTerH0KtlEt3SPbfuth48yWMz17gf/83Z8n5v6S6+3HAA8J9a+MDx4FrcUXZP7pmN+0K70Ma2g573r+rFxdW6//AeRxfYbW8vZ8ifFrhTGfs6T3//ZjvvLd3lzqGfned/DsCirYecdTzpTvMQCicLAy9kXjvH3ihVy1jzwpcnIgxom0Jag1gGd/DuZnnlpp60aZTAu3dUNjDMobElJ82ycl0Pf3UG9N/+dx13vZMFUGFmqsvc9fuDCjzWpQRHvrwoYPma9NiHnovXwSpkoMw5cprTRaU0iIukZwvvNWRX5eQDUOYM6OWXPPxk1R7+uSQbcNwgzi+3tKLL+8t2u7dD2Uov8vP/baUBXakqevu2Pu7tf0/oB8Drt/Tmq0crDu4a2bUp8x65POiEYa60v77yuxwr8HzlPni8sELL8+kfdXL8O2s9/Z7/KuB7uYKZS2FxKSfPlISk+2V0d89N2R2Hglv2b9+xAgb8cT7vLt2FTaTC6lab9zta/a4VqVxJ3PpkOAJ/+Vz7X2w4wKMzVtPn2Xl8tCLX53ta/w/ON38XcCsN6EoF6ca+jq4VayvctchzbJS90lZ8sJLio2jXOMGdkfHZsV1489beju3ZnsnZfZ/7yp3N0ZUquEW5zJTBBuZfj3QkN+vw5Od0eXoumU/MYVfeKU45R+6Ulhn+8PmmGl1dyXruyic188caXK2rQLm4+tHLnOdhSEfH/9ujw9pzfa/0Ct0sj89cw0crczl04gyPfriaqfMd34q6N/e+RxJMYK0OL365hSXbD7v/H4uCuM+gAV2pID03tivbnh0BwL2DWgOQ4GPVo3PVuF6Me8HrDk0S3TdXZ2R5txpdQf+S1skAXv3y4OiHL6tkZEa39PoMaJvic8ji5S98w+UvOIZbrsrJ5+/fbOfe91YAsHjbYf61JJsfdh+ttiBfXFpG/dhIoiJs/LD7KBkTZ/td+cml0Mcasq50x+AJ6KedfewtkuLInjKKfq2SuaRNsrvcL4e18/kN6oW5mwHH509NjObFGxxDHbMP18zC4eW9/NVWfvKPZVz0uy8wxlDk5ya4lQZ0pYIkIkQ4R7A8flV7Vj11ZZUSgQXL2redGBPpNYrDl8ljujB9Qj9apSZ4LdIx7M8LafXrOZzwMzriTHEZcVF2mtaP8Xn88ElHPaKcnzlr11GWZx/hpjeW8fSs9Yx9ZQmjqqnvvaikjOgIG21SE/hopWOayz3vraj0glFgSXbm+txTftyVW/s7Rgu5MmQO/r9vAE9gB+jU1HMDumfLhl4jjKzeWORIsHboxBnaNXYMXb3yzwvd315qivVCfLywhMwn5vCWJe2yPxrQlToLIhLU5J+z8fDQdu7txBjHN4ARXTxDDN+9o69X+aT4KPq1crQ47x3UusJxXyNYJn60hs0HThATaUdE3BkiyztWUEyhZcjk9a9+53W8KjcwK3P45Bki7Tau6dHM695An2fn+e06co1ieWlcd/c3pgi7jd+N6cJjV7Vn//FC/rvKMwcy56ini8X6bSY+KoLOzTxDTF+4rhvxUY4uoGdmbwSgaf0YWqV6nrN+7/Gz/qzB8DWMc2cQ3ww0oCtVywzp2Ni97foG8NL47u59A9qmurd7tPDu33Ud//kVnqX7Nu6rGHymL3ekH4h19l0//aPOvHpzT8CR/vf+wY4AOerlRe7A6c+53kRdt+cY8zYeZE9+ASO6NK1w/K53sljtHLFi5eqWau9j0lebRo55A9bRQr+0zKAt38ViHTJ6fe/mrJ88nPgoO+kNHd+OxvZIIy4qgt+MdMwm/eD73VQXYwyz1+zzOs8FAc65PxrQlaqF3vlZX67u1tTdUoyOsHN9r3T3ZKA/XNuVBnGRfGhJP2D16LD2/H5MZ8Ax6qX8Yhsu1kWxh3dpyoy7+/PkqE7usfW5Rwt89lVbHTrHmabWNVabJ8XRr1WS1/F5Gw8yZupiVuzyHsp53/srAc9FycrVPeLqNhrYLpX4cvc7pk/oR5N6jpZ3m0aJvPHT3iz/zVD38R9d1IzcowWA537FnQMc0+4//sHT8l+35xgbzqHFvnbPMe6ftpKnLbNAXbN3f3Jx1ZIYakBXqhYa2C6Vv/2kp1fAfeH6i3jlJkeeknF9WrDqqWHuPn1fbumf4Xm9F+azN7+gQpn5m7wzdfTNTMJmE1omxzO2RxpN68e4W46u1mp52w6c9Lk/WK7W8hXO0UMf3NWPl8Z1r1DupXlb3dsnLX3YvkYXtUyKIy7K7h7++US5ZQoB+rVKZumvh7i/BQ3t1JhUy4LjF1lGt/z+mi6A4wLYwfmNYNmOPIwxXP3Xb89pHL/rC45rAhPAUeeonVFdm7Lo8cFBB3YN6EqFsc3PDHdvP+vsDwZo6Fyw+q8/6eH3uW0bJ7DvWCFLdzhypbj6mS9rk8Lbt/dh0eODAdi0/9xmmbrWa3Wt7yoi/OiiiiNvoiM8gXtmliejt6/FTWw2oWPTeux35sGpyrKELtf29IyYcS1VCJ41bMe9vpTMJ+a495e/+fzkf9fx6oLt7sd78wt8zlp1TRjae6yQ15zlXfuiI2w0T4rjnoGtg6qzBnSlwpg1CM5eu8+9XVJq+Nmlmd6rNZXT33mj1TVT8tFh7Xl4aDveuq0Pg9s3crfYJ3+6gUmz1nu1mn0pKinzOYzy1BnHNwBrl4jdJtw3qDUvjetO1m+HMqBtCvM2HmB5tqPbpZ5z1md6w1i/31I6WXLp2M9iQYqoCBu3XZJBr5YNibb0uQ8ql3TNZbPlwrY6J593l+5iymebMMbw9uKdXDLla3710Ro+zMrh+c88F1fraJ3nP9sE4E5J4Pr/8/ftqDwN6EqFOWsSLteC1GdKywK2WrukeS8L0SolnoeGtnV3kYgIfZ05av65JJu/f1N5psd2v/3MKz9NaZlhdU6+u4Vevo/78eEduKZHGikJ0e4Lz/WvfsfqnHxKnGOy37LM2i3P2qqOsJ3dWlaTRnfmo3sv8er6Arj78lYVyl736nccPVXkyEs/dbF7/8rd+fzuf45JYTNX5PLYzDW8tmCHe4JS+ZvO73yX7R7lEh3pONe2IOuvAV2pMPfncd0Z3tkR1FfuOsrHP+RSVFIWcFJUpN3G+D6Om6MdmiT6bAm7Rn0ATJ2/3WdWSKuPVuayZJvjojLt+92MmbqY/zjHnfu6uelyjWXhjjFTFzPNOcqkkaXPu7zL2qa4t8unBThXT4zo6O5Xt7rtn8s5Xi49gLVv3GrLAUeLvtAZvF3/R099st59kbN+M5g+oR/v/KwvldGArlSYi46w8+otvchMief/vtjCw/9eDVRsEfviWgHJXz95+VZ8+99+7nMik7Xv+G7njNNiZyDbsO84sZH2SoNui+Q4d64a8CTequwzWCdMucbzV6ebLTcqXX3+q3PyK4whX7HrqM/nf7fdcW/C1UL/+RDPUFPXNxlrl1m/VskMbJdKZTSgK3WBsI61huDSFriGEPrrsrDbhD9df5HXxKeuk76oUM6a/+SEMw3s9zs9wxDjowPnwbn90kxeuamn177ISkb5iAjZU0ax8/mRNTKjV0R4+/Y+/OtnfXl2rKe17rqX8OINF9GhSSKLth72+XzXzWbXrNNGiRVn7Aab1M1FA7pSF4iu5VrTP+4ZeP1REeGjey/hy0d8LhMMwLW90vn7zb144TrPWqg5Rxxjy8+UlDL8pYX8b/Ver+ccLyzm8/X7LY+Dm0o/smtTpt3pSEXcwceEIn+foaYMbt+Iy9ulUi8mkpdvdIwYcn17SG8YR6+WnpS+1u6SLmn1WJWTjzGGXXmniYqw0SAukk2/H+71+tEa0JVSvlgD+of39K90DLtVr5YNKyT+8uX63s3d/cqj//Yty3bk8diHa9i0/4S7C8F1E3Vdrnd64GAyCbpc0iaFdb+7ijkPDgj6OedDT+es3SXOrpSYSJvXMoTWfOx9M5I5fLKI3KMFnDxTQkp8FJF2GzGRdq+hphrQlVI+dUv3TJSpbLjiuXAluTp6uphxry9lVrmWuWtt1h98TOWvioToiKBHfpwvaQ1iaVwvmoVbHDdBYyLtDO3UmLE9HN+EEmMi2PT74fzjp725tpdj38rdR5m5Ipe9lgVJrP3mwV503eXP9UMopeqG2Cg7y38z1J09sabcN6g1r3yz3eexDk3rkdYglhfmbiY6wsZP+7fkH4t2kpLgf7RKXSEijOzalLcXZwMQ4wzML95wEQ8PbefOnX9lp8aUOO8pPDR9lc/X2vzMcM4mRY620JW6gKQmRlM/rvpvEFo9dlV7r8c/7d+SizOTGNKhEdf1SmePMwXBmZIyIuw2Vj81jPm/9N9HX5dcbhmFEue80Ssi7mDuEmG30dqSvbF8/proiLNbMEVb6EqpauUaXTIjK4fE6AhGdPXOoPjLYe34vy+2AJB/urjGLzDn08C2qTSMi+To6WKS4ytPrzxpdGduefN7oOJF8GwFFdBFZDjwF8AOvGGMmVLu+EDgJaAbMN4YM7NaaqeUqrNcGRvLu39wG3dAX7+34tqpdZnNJvzw1LCgyg5om8oTIzpgtwk9mjcM/IRg3j9QARGxA1OBEUAn4EYR6VSu2G7gNmBatdRKKRW2RMSde/3GvlVLDxtu7r68NXcOaFVtN3iDaaH3BbYZY3YAiMh0YAzgXrHWGJPtPHZ+Vk9VStVpw7s0JXvKqFBXI+wEc1M0DcixPM517qsyEZkgIlkiknXokO/8Bkoppc7OeR3lYox53RjT2xjTOzW18pwESimlqiaYgL4HsN7dSHfuU0opVYsEE9CXA21FJFNEooDxwKyarZZSSqmqChjQjTElwAPAXGAjMMMYs15EJovIaAAR6SMiucD1wGsist7/KyqllKoJQY1DN8bMAeaU2/eUZXs5jq4YpZRSIaJT/5VSKkxoQFdKqTChAV0ppcKEBnSllAoTGtCVUipMaEBXSqkwoQFdKaXChAZ0pZQKExrQlVIqTGhAV0qpMKEBXSmlwoQGdKWUChMa0JVSKkxoQFdKqTChAV0ppcKEBnSllAoTGtCVUipMaEBXSqkwoQFdKaXChAZ0pZQKExrQlVIqTAQV0EVkuIhsFpFtIjLRx/FoEfm38/gyEcmo7ooqpZSqXMCALiJ2YCowAugE3CgincoVuwM4aoxpA/wZ+EN1V1QppVTlgmmh9wW2GWN2GGOKgOnAmHJlxgD/cm7PBIaIiFRfNZVSSgUSEUSZNCDH8jgXuNhfGWNMiYgcA5KBw9ZCIjIBmOB8eEZE1p1NpcNcCuXOm3LTc+Obnhf/wvHctPR3IJiAXm2MMa8DrwOISJYxpvf5fP+6QM+Lf3pufNPz4t+Fdm6C6XLZAzS3PE537vNZRkQigPpAXnVUUCmlVHCCCejLgbYikikiUcB4YFa5MrOAW53b1wFfG2NM9VVTKaVUIAG7XJx94g8AcwE78JYxZr2ITAayjDGzgDeBd0VkG3AER9AP5PVzqHc40/Pin54b3/S8+HdBnRvRhrRSSoUHnSmqlFJhQgO6UkqFiZAE9ECpBMKdiGSLyFoRWSUiWc59SSLypYhsdf7b0LlfRORl57laIyI9Q1v76iMib4nIQet8hLM5DyJyq7P8VhG51dd71TV+zs0kEdnj/L1ZJSIjLceecJ6bzSJylWV/WP2tiUhzEZkvIhtEZL2IPOTcr783AMaY8/qD48bqdqAVEAWsBjqd73qE8gfIBlLK7fsjMNG5PRH4g3N7JPAZIEA/YFmo61+N52Eg0BNYd7bnAUgCdjj/bejcbhjqz1ZD52YS8EsfZTs5/46igUzn35c9HP/WgKZAT+d2IrDF+fn198aYkLTQg0klcCGypk/4F3CNZf87xmEp0EBEmoaigtXNGLMQx6goq6qeh6uAL40xR4wxR4EvgeE1X/ua5efc+DMGmG6MOWOM2Qlsw/F3FnZ/a8aYfcaYlc7tE8BGHDPV9feG0HS5+EolkBaCeoSSAb4QkRXOdAgAjY0x+5zb+4HGzu0L7XxV9TxcaOfnAWfXwVuubgUu0HPjzOraA1iG/t4AelM0VC4zxvTEkcHyfhEZaD1oHN8JL/jxpHoeKvg70BroDuwD/hTa6oSOiCQAHwG/MMYctx67kH9vQhHQg0klENaMMXuc/x4EPsbx1fiAqyvF+e9BZ/EL7XxV9TxcMOfHGHPAGFNqjCkD/oHj9wYusHMjIpE4gvn7xpj/OHfr7w2hCejBpBIIWyISLyKJrm1gGLAO7/QJtwKfOLdnAT913q3vBxyzfLUMR1U9D3OBYSLS0NkFMcy5L+yUu3cyFsfvDTjOzXhxLDSTCbQFvicM/9ZERHDMTN9ojHnRckh/b+D8j3IxnjvPW3Dcgf9NqO8Mn+fP3grHaIPVwHrX58eRbvgrYCswD0hy7hccC4xsB9YCvUP9GarxXHyAo+ugGEcf5h1ncx6An+G4EbgNuD3Un6sGz827zs++Bkegamop/xvnudkMjLDsD6u/NeAyHN0pa4BVzp+R+nvj+NGp/0opFSb0pqhSSoUJDehKKRUmNKArpVSY0ICulFJhQgO6UkqFCQ3oSikVJjSgK6VUmPh/rolIS0cZZdIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btDMhztoKem8",
        "colab_type": "text"
      },
      "source": [
        "## INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BfC7lO8Kem-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "6bcacbc7-0448-4259-d1d0-046b4c4816f6"
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    learn.model.roberta.eval()\n",
        "    preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in data.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    ordered_preds = preds[reverse_sampler, :]\n",
        "    pred_values = np.argmax(ordered_preds, axis=1)\n",
        "    return ordered_preds, pred_values\n",
        "\n",
        "\n",
        "test_preds,preds = get_preds_as_nparray(DatasetType.Test)\n",
        "df=pd.DataFrame(data=test_preds,columns=['a','b'])\n",
        "sub['target']=df['b']\n",
        "sub.to_csv('tweet_win.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e12oVJcKKenE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "59c210f2-6d1f-4779-e7b2-7d2c4facebef"
      },
      "source": [
        "sub.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>test_2</td>\n",
              "      <td>0.982313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>test_3</td>\n",
              "      <td>0.001234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>test_4</td>\n",
              "      <td>0.001780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>test_8</td>\n",
              "      <td>0.912273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>test_9</td>\n",
              "      <td>0.001953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>test_10</td>\n",
              "      <td>0.001264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>test_11</td>\n",
              "      <td>0.127850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>test_14</td>\n",
              "      <td>0.155842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>test_16</td>\n",
              "      <td>0.988733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>test_19</td>\n",
              "      <td>0.682924</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        ID    target\n",
              "0   test_2  0.982313\n",
              "1   test_3  0.001234\n",
              "2   test_4  0.001780\n",
              "3   test_8  0.912273\n",
              "4   test_9  0.001953\n",
              "5  test_10  0.001264\n",
              "6  test_11  0.127850\n",
              "7  test_14  0.155842\n",
              "8  test_16  0.988733\n",
              "9  test_19  0.682924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbABn4YC8bVy",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://analyticsindiamag.com/wp-content/uploads/2019/12/Transformer-04-scaled.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1vbyTY08f0X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1ae7b5b3-69a7-4bae-d1c4-17998e8e2ee3"
      },
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 25.2MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 39.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a72a279d1609853e953741ec7f6496e84a7ea98e936d251c7e0db45e83cd65e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Collecting simpletransformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f6/e50e3ac46e931e91e679e880170bd68afb159b2dfb925cdf629bf16a09e2/simpletransformers-0.43.5-py3-none-any.whl (194kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.7MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.47.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/62/7663894f67ac5a41a0d8812d78d9d2a9404124051885af9d77dc526fb399/tqdm-4.47.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers>=2.11.0 in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (3.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.22.2.post1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2019.12.20)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.0.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.4.1)\n",
            "Collecting tensorboardx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (1.18.5)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.6/dist-packages (from simpletransformers) (0.8.1rc1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (0.0.43)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (20.4)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->simpletransformers) (0.1.91)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->simpletransformers) (0.15.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->simpletransformers) (2.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->simpletransformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardx->simpletransformers) (3.10.0)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->simpletransformers) (2.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.11.0->simpletransformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=2.11.0->simpletransformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (47.3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->simpletransformers) (1.0.8)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=ccdd2aecafff14b0dcc4867e332d51091847757af92948293cfbed7c8244ab0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tqdm, tensorboardx, seqeval, simpletransformers\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed seqeval-0.0.12 simpletransformers-0.43.5 tensorboardx-2.1 tqdm-4.47.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rmLHIsKBTT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "import sklearn\n",
        "from sklearn.metrics import log_loss\n",
        "import re\n",
        "import random\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "from simpletransformers.classification import ClassificationModel\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOn244c2BTs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=pd.read_csv('/content/drive/My Drive/tweet/updated_train.csv')\n",
        "test=pd.read_csv('/content/drive/My Drive/tweet/updated_test.csv')\n",
        "\n",
        "train1=train.drop(['ID'],axis=1)\n",
        "test1=test.drop(['ID'],axis=1)\n",
        "test1['label']=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzbqvXlEBTvr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "%%time\n",
        "err=[]\n",
        "y_pred_tot=[]\n",
        "\n",
        "fold=StratifiedKFold(n_splits=10, shuffle=True, random_state=2)\n",
        "i=1\n",
        "for train_index, test_index in fold.split(train1,train1['target']):\n",
        "    train1_trn, train1_val = train1.iloc[train_index], train1.iloc[test_index]\n",
        "    model = ClassificationModel('roberta', 'roberta-large', use_cuda=True,num_labels=2, args={'train_batch_size':32,\n",
        "                                                                         'reprocess_input_data': True,\n",
        "                                                                         'overwrite_output_dir': True,\n",
        "                                                                         'fp16': False,\n",
        "                                                                         'do_lower_case': False,\n",
        "                                                                         'num_train_epochs': 2,\n",
        "                                                                         'max_seq_length': 64,\n",
        "                                                                         'regression': False,\n",
        "                                                                         'manual_seed': 2,\n",
        "                                                                         \"learning_rate\":3e-5,\n",
        "                                                                         'weight_decay':0,\n",
        "                                                                         \"save_eval_checkpoints\": False,\n",
        "                                                                         \"save_model_every_epoch\": False,\n",
        "                                                                         \"silent\": True})\n",
        "    model.train_model(train1_trn)\n",
        "    raw_outputs_val = model.eval_model(train1_val)[1]\n",
        "    raw_outputs_val = softmax(raw_outputs_val,axis=1)[:,1]\n",
        "    print(f\"Log_Loss: {log_loss(train1_val['target'], raw_outputs_val)}\")\n",
        "    err.append(log_loss(train1_val['target'], raw_outputs_val))\n",
        "    raw_outputs_test = model.eval_model(test1)[1]\n",
        "    raw_outputs_test = softmax(raw_outputs_test,axis=1)[:,1]\n",
        "    y_pred_tot.append(raw_outputs_test)\n",
        "print(\"Mean LogLoss: \",np.mean(err))\n",
        "sub=pd.DataFrame()\n",
        "sub['ID']=test['ID']\n",
        "sub['target']=np.mean(y_pred_test, 0)\n",
        "sub.to_csv('simple_transformers1.csv',index=False)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZSqJuCEBTyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}